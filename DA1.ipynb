{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Programs using Spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinguishing between Nouns and Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Entity Relations (plac ?)\n",
    "\n",
    "\n",
    "#### Extract money and currency values and then check the dependency tree to find the referred noun phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/examples\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model to load (needs parser and NER)\", \"positional\", None, str)\n",
    ")   \n",
    "# Ask ma'am for doubt ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=\"en_core_web_sm\"):\n",
    "    nlp = spacy.load(model)\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "    print(\"Processing %d texts\" % len(TEXTS))\n",
    "\n",
    "    for text in TEXTS:\n",
    "        doc = nlp(text)\n",
    "        relations = extract_currency_relations(doc)\n",
    "        for r1, r2 in relations:\n",
    "            print(\"{:<10}\\t{}\\t{}\".format(r1.text, r2.ent_type_, r2.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spans(spans):\n",
    "    # Filter a sequence of spans so they don't contain overlaps\n",
    "    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n",
    "    get_sort_key = lambda span: (span.end - span.start, -span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_currency_relations(doc):\n",
    "    # Merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    spans = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in spans:\n",
    "            retokenizer.merge(span)\n",
    "\n",
    "    relations = []\n",
    "    for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc):\n",
    "        if money.dep_ in (\"attr\", \"dobj\"):\n",
    "            subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                relations.append((subject, money))\n",
    "        elif money.dep_ == \"pobj\" and money.head.dep_ == \"prep\":\n",
    "            relations.append((money.head.head, money))\n",
    "    return relations\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Matcher\n",
    "\n",
    "#### Match a large set of multi-word expressions in O(1) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/examples\n",
    "from __future__ import print_function, unicode_literals, division\n",
    "\n",
    "from bz2 import BZ2File\n",
    "import time\n",
    "import plac\n",
    "import json\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    patterns_loc=(\"Path to gazetteer\", \"positional\", None, str),\n",
    "    text_loc=(\"Path to Reddit corpus file\", \"positional\", None, str),\n",
    "    n=(\"Number of texts to read\", \"option\", \"n\", int),\n",
    "    lang=(\"Language class to initialise\", \"option\", \"l\", str),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(patterns_loc, text_loc, n=10000, lang=\"en\"):\n",
    "    nlp = spacy.blank(lang)\n",
    "    nlp.vocab.lex_attr_getters = {}\n",
    "    phrases = read_gazetteer(nlp.tokenizer, patterns_loc)\n",
    "    count = 0\n",
    "    t1 = time.time()\n",
    "    for ent_id, text in get_matches(nlp.tokenizer, phrases, read_text(text_loc, n=n)):\n",
    "        count += 1\n",
    "    t2 = time.time()\n",
    "    print(\"%d docs in %.3f s. %d matches\" % (n, (t2 - t1), count))\n",
    "\n",
    "\n",
    "def read_gazetteer(tokenizer, loc, n=-1):\n",
    "    for i, line in enumerate(open(loc)):\n",
    "        data = json.loads(line.strip())\n",
    "        phrase = tokenizer(data[\"text\"])\n",
    "        for w in phrase:\n",
    "            _ = tokenizer.vocab[w.text]\n",
    "        if len(phrase) >= 2:\n",
    "            yield phrase\n",
    "\n",
    "\n",
    "def read_text(bz2_loc, n=10000):\n",
    "    with BZ2File(bz2_loc) as file_:\n",
    "        for i, line in enumerate(file_):\n",
    "            data = json.loads(line)\n",
    "            yield data[\"body\"]\n",
    "            if i >= n:\n",
    "                break\n",
    "\n",
    "\n",
    "def get_matches(tokenizer, phrases, texts):\n",
    "    matcher = PhraseMatcher(tokenizer.vocab)\n",
    "    matcher.add(\"Phrase\", None, *phrases)\n",
    "    for text in texts:\n",
    "        doc = tokenizer(text)\n",
    "        for w in doc:\n",
    "            _ = doc.vocab[w.text]\n",
    "        matches = matcher(doc)\n",
    "        for ent_id, start, end in matches:\n",
    "            yield (ent_id, doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if False:\n",
    "        import cProfile\n",
    "        import pstats\n",
    "\n",
    "        cProfile.runctx(\"plac.call(main)\", globals(), locals(), \"Profile.prof\")\n",
    "        s = pstats.Stats(\"Profile.prof\")\n",
    "        s.strip_dirs().sort_stats(\"time\").print_stats()\n",
    "    else:\n",
    "        plac.call(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the parse tree and subtrees\n",
    "\n",
    "#### This example shows how to navigate the parse tree including subtrees attached to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/examples\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import spacy\n",
    "\n",
    "\n",
    "@plac.annotations(model=(\"Model to load\", \"positional\", None, str))\n",
    "def main(model=\"en_core_web_sm\"):\n",
    "    nlp = spacy.load(model)\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "\n",
    "    doc = nlp(\n",
    "        \"displaCy uses CSS and JavaScript to show you how computers \"\n",
    "        \"understand language\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "        if word.dep_ in (\"xcomp\", \"ccomp\"):\n",
    "            print(\"\".join(w.text_with_ws for w in word.subtree))\n",
    "for word in doc:\n",
    "        if word.dep_ in (\"xcomp\", \"ccomp\"):\n",
    "            subtree_span = doc[word.left_edge.i : word.right_edge.i + 1]\n",
    "            print(subtree_span.text, \"|\", subtree_span.root.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline components via REST API\n",
    "\n",
    "#### Pipeline component that requests all countries via the REST Countries API, merges country names into one token, assigns entity labels and sets attributes on country tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/examples\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import requests\n",
    "import plac\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # For simplicity, we start off with only the blank English Language class\n",
    "    # and no model or pre-defined pipeline loaded.\n",
    "    nlp = English()\n",
    "    rest_countries = RESTCountriesComponent(nlp)  # initialise component\n",
    "    nlp.add_pipe(rest_countries)  # add it to the pipeline\n",
    "    doc = nlp(\"Some text about Colombia and the Czech Republic\")\n",
    "    print(\"Pipeline\", nlp.pipe_names)  # pipeline contains component name\n",
    "    print(\"Doc has countries\", doc._.has_country)  # Doc contains countries\n",
    "    for token in doc:\n",
    "        if token._.is_country:\n",
    "            print(\n",
    "                token.text,\n",
    "                token._.country_capital,\n",
    "                token._.country_latlng,\n",
    "                token._.country_flag,\n",
    "            )  # country data\n",
    "    print(\"Entities\", [(e.text, e.label_) for e in doc.ents])  # entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESTCountriesComponent(object):\n",
    "    name = \"rest_countries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def __init__(self, nlp, label=\"GPE\"):\n",
    "        \"\"\"Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the shared vocab, get the label ID and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \"\"\"\n",
    "        # Make request once on initialisation and store the data\n",
    "        r = requests.get(\"https://restcountries.eu/rest/v2/all\")\n",
    "        r.raise_for_status()  # make sure requests raises an error if it fails\n",
    "        countries = r.json()\n",
    "\n",
    "        # Convert API response to dict keyed by country name for easy lookup\n",
    "        # This could also be extended using the alternative and foreign language\n",
    "        # names provided by the API\n",
    "        self.countries = {c[\"name\"]: c for c in countries}\n",
    "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
    "\n",
    "        # Set up the PhraseMatcher with Doc patterns for each country name\n",
    "        patterns = [nlp(c) for c in self.countries.keys()]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(\"COUNTRIES\", None, *patterns)\n",
    "\n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        # If no default value is set, it defaults to None.\n",
    "        Token.set_extension(\"is_country\", default=False)\n",
    "        Token.set_extension(\"country_capital\", default=False)\n",
    "        Token.set_extension(\"country_latlng\", default=False)\n",
    "        Token.set_extension(\"country_flag\", default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_country == True.\n",
    "        Doc.set_extension(\"has_country\", getter=self.has_country)\n",
    "        Span.set_extension(\"has_country\", getter=self.has_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity & set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            # Can be extended with other data returned by the API, like\n",
    "            # currencies, country code, flag, calling code etc.\n",
    "            for token in entity:\n",
    "                token._.set(\"is_country\", True)\n",
    "                token._.set(\"country_capital\", self.countries[entity.text][\"capital\"])\n",
    "                token._.set(\"country_latlng\", self.countries[entity.text][\"latlng\"])\n",
    "                token._.set(\"country_flag\", self.countries[entity.text][\"flag\"])\n",
    "            # Overwrite doc.ents and add entity – be careful not to replace!\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token. This is done\n",
    "            # after setting the entities – otherwise, it would cause mismatched\n",
    "            # indices!\n",
    "            span.merge()\n",
    "        return doc  # don't forget to return the Doc!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def has_country(self, tokens):\n",
    "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
    "        is a country. Since the getter is only called when we access the\n",
    "        attribute, we can refer to the Token's 'is_country' attribute here,\n",
    "        which is already set in the processing step.\"\"\"\n",
    "        return any([t._.get(\"is_country\") for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://realpython.com/natural-language-processing-spacy-python/\n",
    "\n",
    "Integration of Spacy in Python IDLE\n",
    "\n",
    "#### https://www.datacamp.com/community/blog/spacy-cheatsheet\n",
    "\n",
    "Detailed review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
